# Plan integracji LLM (Lite) w Broxeen

## Cel

DodaÄ‡ do Broxeen multimodalny LLM (via OpenRouter) obsÅ‚ugujÄ…cy:
- **Opis grafiki** â€” co widaÄ‡ na stronie (screenshoty, obrazki)
- **Q&A po treÅ›ci** â€” odpowiadanie na pytania o zawartoÅ›Ä‡ strony
- **PeÅ‚ny STT â†’ LLM â†’ TTS** â€” mÃ³wisz pytanie, dostajesz gÅ‚osowÄ… odpowiedÅº
- **Inteligentne przeglÄ…danie** â€” LLM analizuje HTML i wyciÄ…ga sens

## Architektura

```
 Mikrofon (STT)          Klawiatura
      â”‚                      â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
      useSpeech.ts (istniejÄ…cy)
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Chat.tsx       â”‚  â† decyduje: browse? pytanie do LLM? opis strony?
    â”‚   (orchestrator) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼       â–¼            â–¼
 resolver  browse     llmGateway.ts â”€â”€â†’ Tauri cmd â”€â”€â†’ OpenRouter API
  (URL)    (fetch)       â”‚                               â”‚
                         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼         â–¼
                    odpowiedÅº tekstowa
                         â”‚
                         â–¼
                    useTts.ts (istniejÄ…cy) â†’ GÅ‚oÅ›nik
```

## Nowe pliki

```
broxeen/
â”œâ”€â”€ .env                              # NOWY â€” konfiguracja LLM
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ llmClient.ts              # NOWY â€” klient OpenRouter (TS)
â”‚   â”‚   â”œâ”€â”€ llmClient.test.ts         # NOWY â€” testy
â”‚   â”‚   â””â”€â”€ llmPrompts.ts             # NOWY â€” system prompty per tryb
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useLlm.ts                 # NOWY â€” React hook do LLM
â”‚   â”‚   â””â”€â”€ useLlm.test.ts            # NOWY â€” testy
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ Chat.tsx                  # ZMIANA â€” integracja LLM w handleSubmit
â”œâ”€â”€ src-tauri/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.rs                   # ZMIANA â€” nowe komendy Tauri
â”‚       â”œâ”€â”€ llm.rs                    # NOWY â€” moduÅ‚ Rust do OpenRouter
â”‚       â””â”€â”€ screenshot.rs             # NOWY â€” screenshot WebView â†’ base64
â””â”€â”€ python/                           # OPCJONALNY sidecar
    â””â”€â”€ llm_client.py                 # TwÃ³j istniejÄ…cy klient (backup)
```

## Konfiguracja

### `.env`

```env
# â”€â”€ LLM via OpenRouter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENROUTER_API_KEY=sk-or-v1-3afad9d16461cb...
LLM_MODEL=google/gemini-3-flash-preview
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7

# â”€â”€ Broxeen-specific â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LLM_SYSTEM_PROMPT_BROWSE=JesteÅ› asystentem przeglÄ…dania internetu. Odpowiadaj po polsku, zwiÄ™Åºle.
LLM_SYSTEM_PROMPT_DESCRIBE=Opisz co widzisz na obrazku. Odpowiadaj po polsku.
LLM_SYSTEM_PROMPT_QA=Odpowiedz na pytanie na podstawie podanej treÅ›ci strony. BÄ…dÅº zwiÄ™zÅ‚y.
```

### `src-tauri/tauri.conf.json` â€” dodaj env

```jsonc
{
  "build": {
    "beforeDevCommand": "npm run dev",
    "devUrl": "http://localhost:1420",
    "beforeBuildCommand": "npm run build"
  },
  // ... reszta bez zmian
}
```

---

## PrzykÅ‚adowe pliki implementacji

### 1. `src/lib/llmClient.ts` â€” klient OpenRouter

```typescript
/**
 * llmClient â€” Unified LLM client via OpenRouter for Broxeen.
 * Works both in browser (dev) and through Tauri commands (production).
 */

import { isTauriRuntime } from "./runtime";
import { createScopedLogger } from "./logger";

const log = createScopedLogger("llm");

const OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions";

export interface LlmConfig {
  apiKey: string;
  model: string;
  maxTokens: number;
  temperature: number;
}

export interface LlmMessage {
  role: "system" | "user" | "assistant";
  content: string | LlmContentPart[];
}

/** Multimodal content (text + images) */
export type LlmContentPart =
  | { type: "text"; text: string }
  | { type: "image_url"; image_url: { url: string } };

export interface LlmResponse {
  text: string;
  model: string;
  usage?: { prompt_tokens: number; completion_tokens: number };
}

/** Load config from env or Tauri settings */
export function getConfig(): LlmConfig {
  return {
    apiKey: import.meta.env.VITE_OPENROUTER_API_KEY ?? "",
    model: import.meta.env.VITE_LLM_MODEL ?? "google/gemini-3-flash-preview",
    maxTokens: Number(import.meta.env.VITE_LLM_MAX_TOKENS ?? 2048),
    temperature: Number(import.meta.env.VITE_LLM_TEMPERATURE ?? 0.7),
  };
}

/**
 * Send a chat completion request to OpenRouter.
 * Supports text-only and multimodal (image) messages.
 */
export async function chat(
  messages: LlmMessage[],
  config?: Partial<LlmConfig>
): Promise<LlmResponse> {
  const cfg = { ...getConfig(), ...config };

  if (!cfg.apiKey) {
    throw new Error("OPENROUTER_API_KEY not set. Configure in .env file.");
  }

  // In Tauri runtime, delegate to Rust backend (bypasses CORS)
  if (isTauriRuntime()) {
    return chatViaTauri(messages, cfg);
  }

  // Browser fallback (dev mode)
  return chatDirect(messages, cfg);
}

async function chatDirect(
  messages: LlmMessage[],
  cfg: LlmConfig
): Promise<LlmResponse> {
  log.info(`Sending ${messages.length} messages to ${cfg.model}`);

  const resp = await fetch(OPENROUTER_URL, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${cfg.apiKey}`,
      "Content-Type": "application/json",
      "HTTP-Referer": "https://broxeen.local",
      "X-Title": "broxeen",
    },
    body: JSON.stringify({
      model: cfg.model,
      messages,
      max_tokens: cfg.maxTokens,
      temperature: cfg.temperature,
    }),
  });

  if (!resp.ok) {
    const body = await resp.text();
    throw new Error(`LLM HTTP ${resp.status}: ${body.slice(0, 200)}`);
  }

  const data = await resp.json();
  return {
    text: data.choices[0].message.content,
    model: data.model,
    usage: data.usage,
  };
}

async function chatViaTauri(
  messages: LlmMessage[],
  cfg: LlmConfig
): Promise<LlmResponse> {
  const { invoke } = await import("@tauri-apps/api/core");
  return invoke<LlmResponse>("llm_chat", { messages, config: cfg });
}

// â”€â”€ Convenience wrappers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/** Ask LLM a question about page content */
export async function askAboutContent(
  pageContent: string,
  question: string
): Promise<string> {
  const messages: LlmMessage[] = [
    {
      role: "system",
      content:
        "JesteÅ› asystentem przeglÄ…dania internetu Broxeen. " +
        "Odpowiadaj po polsku, zwiÄ™Åºle i na temat. " +
        "UÅ¼ytkownik przeglÄ…da stronÄ™ i zadaje pytanie o jej treÅ›Ä‡.",
    },
    {
      role: "user",
      content: `TreÅ›Ä‡ strony:\n\n${pageContent.slice(0, 6000)}\n\n---\nPytanie: ${question}`,
    },
  ];
  const resp = await chat(messages);
  return resp.text;
}

/** Describe an image (screenshot or page image) */
export async function describeImage(
  base64Image: string,
  mimeType: string = "image/png",
  prompt: string = "Opisz dokÅ‚adnie co widzisz na tym obrazku. Odpowiedz po polsku."
): Promise<string> {
  const messages: LlmMessage[] = [
    {
      role: "system",
      content:
        "JesteÅ› asystentem wizualnym. Opisujesz obrazki i screenshoty stron internetowych po polsku.",
    },
    {
      role: "user",
      content: [
        { type: "text", text: prompt },
        {
          type: "image_url",
          image_url: {
            url: `data:${mimeType};base64,${base64Image}`,
          },
        },
      ],
    },
  ];
  const resp = await chat(messages);
  return resp.text;
}

/** Summarize page content for TTS readout */
export async function summarizeForTts(
  pageContent: string,
  maxSentences: number = 5
): Promise<string> {
  const messages: LlmMessage[] = [
    {
      role: "system",
      content:
        `Podsumuj poniÅ¼szÄ… treÅ›Ä‡ strony w maksymalnie ${maxSentences} zdaniach. ` +
        "Pisz naturalnym jÄ™zykiem polskim, tak Å¼eby dobrze brzmiaÅ‚o czytane na gÅ‚os przez syntezator mowy. " +
        "Nie uÅ¼ywaj markdown, linkÃ³w ani formatowania.",
    },
    {
      role: "user",
      content: pageContent.slice(0, 8000),
    },
  ];
  const resp = await chat(messages);
  return resp.text;
}
```

### 2. `src/lib/llmPrompts.ts` â€” prompty systemowe

```typescript
/**
 * System prompts for different Broxeen LLM modes.
 * Centralized for easy tuning.
 */

export const PROMPTS = {
  /** Tryb przeglÄ…dania â€” streszczanie stron */
  browse:
    "JesteÅ› asystentem przeglÄ…dania internetu Broxeen. " +
    "UÅ¼ytkownik mÃ³wi po polsku i przeglÄ…da strony przez chat. " +
    "Streszczaj treÅ›Ä‡ strony zwiÄ™Åºle, naturalnym jÄ™zykiem polskim. " +
    "Nie uÅ¼ywaj markdown. Pisz tak, by syntezator mowy brzmiaÅ‚ naturalnie.",

  /** Tryb Q&A â€” pytania o treÅ›Ä‡ */
  qa:
    "Odpowiadaj na pytania o treÅ›Ä‡ strony internetowej. " +
    "BÄ…dÅº zwiÄ™zÅ‚y i konkretny. Odpowiadaj po polsku. " +
    "JeÅ›li odpowiedzi nie ma w treÅ›ci, powiedz o tym.",

  /** Tryb opisu grafiki */
  vision:
    "Opisujesz obrazki i screenshoty stron internetowych. " +
    "Opisz ukÅ‚ad strony, widoczne elementy, tekst i grafiki. " +
    "Odpowiadaj po polsku, zwiÄ™Åºle.",

  /** Tryb identyfikacji intencji uÅ¼ytkownika */
  intent:
    "OkreÅ›l intencjÄ™ uÅ¼ytkownika. Odpowiedz JEDNYM sÅ‚owem:\n" +
    "- BROWSE â€” chce otworzyÄ‡ stronÄ™ (podaÅ‚ URL lub nazwÄ™)\n" +
    "- ASK â€” zadaje pytanie o obecnÄ… stronÄ™\n" +
    "- DESCRIBE â€” chce opis tego co widzi\n" +
    "- SEARCH â€” chce szukaÄ‡ czegoÅ› w internecie\n" +
    "- COMMAND â€” komenda systemowa (np. gÅ‚oÅ›niej, ciszej, stop)\n" +
    "- CHAT â€” zwykÅ‚a rozmowa\n" +
    "Odpowiedz TYLKO jednym sÅ‚owem.",

  /** Tryb ekstrakcji treÅ›ci z HTML */
  extract:
    "WyciÄ…gnij najwaÅ¼niejszÄ… treÅ›Ä‡ z podanego HTML. " +
    "Ignoruj nawigacjÄ™, reklamy, stopki. " +
    "ZwrÃ³Ä‡ czysty tekst artykuÅ‚u / gÅ‚Ã³wnej treÅ›ci.",
} as const;

export type PromptMode = keyof typeof PROMPTS;
```

### 3. `src/hooks/useLlm.ts` â€” React hook

```typescript
import { useState, useCallback, useRef } from "react";
import { chat, askAboutContent, describeImage, summarizeForTts } from "../lib/llmClient";
import { PROMPTS, PromptMode } from "../lib/llmPrompts";
import type { LlmMessage } from "../lib/llmClient";
import { createScopedLogger } from "../lib/logger";

const log = createScopedLogger("useLlm");

interface UseLlmOptions {
  /** Kontekst strony do Q&A */
  pageContent?: string;
  /** Historia konwersacji */
  maxHistory?: number;
}

interface UseLlmReturn {
  /** WyÅ›lij wiadomoÅ›Ä‡ tekstowÄ… */
  send: (text: string) => Promise<string>;
  /** Opisz obraz (base64) */
  describe: (base64: string, mime?: string) => Promise<string>;
  /** Streszcz treÅ›Ä‡ strony dla TTS */
  summarize: (content: string) => Promise<string>;
  /** Wykryj intencjÄ™ uÅ¼ytkownika */
  detectIntent: (text: string) => Promise<PromptMode>;
  /** Stan Å‚adowania */
  loading: boolean;
  /** Ostatni bÅ‚Ä…d */
  error: string | null;
  /** Historia konwersacji */
  history: LlmMessage[];
  /** WyczyÅ›Ä‡ historiÄ™ */
  clearHistory: () => void;
}

export function useLlm(options: UseLlmOptions = {}): UseLlmReturn {
  const { pageContent, maxHistory = 20 } = options;
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const historyRef = useRef<LlmMessage[]>([]);

  const send = useCallback(
    async (text: string): Promise<string> => {
      setLoading(true);
      setError(null);
      try {
        let result: string;

        if (pageContent) {
          // Q&A mode â€” pytanie o treÅ›Ä‡ strony
          result = await askAboutContent(pageContent, text);
        } else {
          // General chat mode
          const messages: LlmMessage[] = [
            { role: "system", content: PROMPTS.browse },
            ...historyRef.current.slice(-maxHistory),
            { role: "user", content: text },
          ];
          const resp = await chat(messages);
          result = resp.text;
        }

        // Update history
        historyRef.current.push(
          { role: "user", content: text },
          { role: "assistant", content: result }
        );

        return result;
      } catch (e: any) {
        const msg = e.message ?? String(e);
        log.error(msg);
        setError(msg);
        return `BÅ‚Ä…d LLM: ${msg}`;
      } finally {
        setLoading(false);
      }
    },
    [pageContent, maxHistory]
  );

  const describe = useCallback(async (base64: string, mime = "image/png") => {
    setLoading(true);
    setError(null);
    try {
      return await describeImage(base64, mime);
    } catch (e: any) {
      setError(e.message);
      return `BÅ‚Ä…d opisu: ${e.message}`;
    } finally {
      setLoading(false);
    }
  }, []);

  const summarize = useCallback(async (content: string) => {
    setLoading(true);
    setError(null);
    try {
      return await summarizeForTts(content);
    } catch (e: any) {
      setError(e.message);
      return `BÅ‚Ä…d streszczenia: ${e.message}`;
    } finally {
      setLoading(false);
    }
  }, []);

  const detectIntent = useCallback(async (text: string): Promise<PromptMode> => {
    try {
      const messages: LlmMessage[] = [
        { role: "system", content: PROMPTS.intent },
        { role: "user", content: text },
      ];
      const resp = await chat(messages, { maxTokens: 10, temperature: 0.1 });
      const intent = resp.text.trim().toLowerCase();

      const validIntents: Record<string, PromptMode> = {
        browse: "browse",
        ask: "qa",
        describe: "vision",
        search: "browse",
        command: "browse",
        chat: "browse",
      };
      return validIntents[intent] ?? "browse";
    } catch {
      return "browse";
    }
  }, []);

  const clearHistory = useCallback(() => {
    historyRef.current = [];
  }, []);

  return {
    send,
    describe,
    summarize,
    detectIntent,
    loading,
    error,
    history: historyRef.current,
    clearHistory,
  };
}
```

### 4. `src-tauri/src/llm.rs` â€” backend Rust

```rust
//! LLM module â€” OpenRouter API client for Tauri backend.
//! Handles API calls server-side to avoid CORS and protect API key.

use serde::{Deserialize, Serialize};
use std::env;

const OPENROUTER_URL: &str = "https://openrouter.ai/api/v1/chat/completions";

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LlmConfig {
    pub api_key: String,
    pub model: String,
    pub max_tokens: u32,
    pub temperature: f32,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(untagged)]
pub enum MessageContent {
    Text(String),
    Parts(Vec<ContentPart>),
}

#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(tag = "type")]
pub enum ContentPart {
    #[serde(rename = "text")]
    Text { text: String },
    #[serde(rename = "image_url")]
    ImageUrl { image_url: ImageUrlData },
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ImageUrlData {
    pub url: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct LlmMessage {
    pub role: String,
    pub content: MessageContent,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LlmResponse {
    pub text: String,
    pub model: String,
}

/// Load LLM config from environment
pub fn get_config() -> LlmConfig {
    LlmConfig {
        api_key: env::var("OPENROUTER_API_KEY").unwrap_or_default(),
        model: env::var("LLM_MODEL")
            .unwrap_or_else(|_| "google/gemini-3-flash-preview".into()),
        max_tokens: env::var("LLM_MAX_TOKENS")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(2048),
        temperature: env::var("LLM_TEMPERATURE")
            .ok()
            .and_then(|v| v.parse().ok())
            .unwrap_or(0.7),
    }
}

/// Send chat completion to OpenRouter
pub async fn chat_completion(
    messages: Vec<LlmMessage>,
    config: Option<LlmConfig>,
) -> Result<LlmResponse, String> {
    let cfg = config.unwrap_or_else(get_config);

    if cfg.api_key.is_empty() {
        return Err("OPENROUTER_API_KEY not set".into());
    }

    let payload = serde_json::json!({
        "model": cfg.model,
        "messages": messages,
        "max_tokens": cfg.max_tokens,
        "temperature": cfg.temperature,
    });

    let client = reqwest::Client::new();
    let resp = client
        .post(OPENROUTER_URL)
        .header("Authorization", format!("Bearer {}", cfg.api_key))
        .header("Content-Type", "application/json")
        .header("HTTP-Referer", "https://broxeen.local")
        .header("X-Title", "broxeen")
        .json(&payload)
        .send()
        .await
        .map_err(|e| format!("Request failed: {e}"))?;

    if !resp.status().is_success() {
        let status = resp.status();
        let body = resp.text().await.unwrap_or_default();
        return Err(format!("HTTP {status}: {}", &body[..body.len().min(200)]));
    }

    let data: serde_json::Value = resp
        .json()
        .await
        .map_err(|e| format!("Parse error: {e}"))?;

    let text = data["choices"][0]["message"]["content"]
        .as_str()
        .unwrap_or("")
        .to_string();

    let model = data["model"].as_str().unwrap_or("").to_string();

    Ok(LlmResponse { text, model })
}
```

### 5. `src-tauri/src/main.rs` â€” nowe komendy Tauri

```rust
// Dodaj do istniejÄ…cego main.rs:

mod llm;

/// Tauri command: LLM chat completion
#[tauri::command]
async fn llm_chat(
    messages: Vec<llm::LlmMessage>,
    config: Option<llm::LlmConfig>,
) -> Result<llm::LlmResponse, String> {
    llm::chat_completion(messages, config).await
}

/// Tauri command: Screenshot current WebView as base64
#[tauri::command]
async fn screenshot_webview(
    window: tauri::Window,
) -> Result<String, String> {
    // Tauri 2 doesn't have built-in screenshot yet
    // Option A: Use JS to capture canvas
    // Option B: Use platform-specific screenshot
    Err("Screenshot not yet implemented â€” use JS canvas capture".into())
}

// W .build() dodaj:
//   .invoke_handler(tauri::generate_handler![
//       browse, get_settings, save_settings,
//       llm_chat, screenshot_webview,   // â† NOWE
//   ])
```

### 6. Zmiany w `src/components/Chat.tsx`

```typescript
// â”€â”€ Dodaj importy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import { useLlm } from "../hooks/useLlm";
import { PROMPTS } from "../lib/llmPrompts";

// â”€â”€ W komponencie Chat: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const [pageContent, setPageContent] = useState<string>("");

const llm = useLlm({ pageContent });

// â”€â”€ ZmieÅ„ handleSubmit: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function handleSubmit(text?: string) {
  const input = text ?? inputText.trim();
  if (!input) return;

  addMessage({ role: "user", text: input });
  setInputText("");

  // 1. SprÃ³buj resolver (istniejÄ…ca logika)
  const resolved = resolve(input);

  if (resolved.kind === "exact" || resolved.kind === "fuzzy") {
    // Browse + LLM summarize
    addMessage({ role: "system", text: "Otwieram stronÄ™..." });
    const browseResult = await executeBrowseCommand(resolved.url, runtimeIsTauri);
    setPageContent(browseResult.content);

    // LLM streszczenie dla TTS
    const summary = await llm.summarize(browseResult.content);
    addMessage({ role: "assistant", text: summary });

    // Auto-TTS jeÅ›li wÅ‚Ä…czone
    if (settings.tts_enabled) {
      speak(summary);
    }
  } else if (pageContent && !looksLikeUrl(input)) {
    // Q&A o aktualnej stronie
    addMessage({ role: "system", text: "MyÅ›lÄ™..." });
    const answer = await llm.send(input);
    updateLastSystemMessage(answer);

    if (settings.tts_enabled) {
      speak(answer);
    }
  } else if (resolved.kind === "search") {
    // Wyszukiwanie
    // ... istniejÄ…ca logika DuckDuckGo ...
  }
}
```

### 7. `src-tauri/Cargo.toml` â€” nowe zaleÅ¼noÅ›ci

```toml
# Dodaj do [dependencies]:
reqwest = { version = "0.12", features = ["json"] }
base64 = "0.22"
# serde i serde_json juÅ¼ powinny byÄ‡
```

### 8. `.env` (peÅ‚ny)

```env
# â”€â”€ OpenRouter LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VITE_OPENROUTER_API_KEY=sk-or-v1-3afad9d16461cb...
VITE_LLM_MODEL=google/gemini-3-flash-preview
VITE_LLM_MAX_TOKENS=2048
VITE_LLM_TEMPERATURE=0.7

# â”€â”€ Tauri backend (bez VITE_ prefix) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENROUTER_API_KEY=sk-or-v1-3afad9d16461cb...
LLM_MODEL=google/gemini-3-flash-preview
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.7
```

> **Uwaga:** `VITE_` prefix = dostÄ™pne w frontend. Bez `VITE_` = tylko backend Rust.
> W produkcji klucz API powinien byÄ‡ TYLKO po stronie Rust.

---

## Flow peÅ‚nego cyklu STT â†’ LLM â†’ TTS

```
1. UÅ¼ytkownik klika mikrofon (toggleMic)
2. useSpeech.ts â†’ rozpoznaje mowÄ™ â†’ "co jest na tej stronie"
3. Chat.tsx â†’ onTranscript â†’ handleSubmit("co jest na tej stronie")
4. handleSubmit:
   a. pageContent istnieje â†’ tryb Q&A
   b. useLlm.send("co jest na tej stronie") â†’ OpenRouter API
   c. LLM odpowiada: "Na stronie jest artykuÅ‚ o..."
   d. addMessage({ role: "assistant", text: odpowiedÅº })
   e. useTts.speak(odpowiedÅº) â†’ gÅ‚oÅ›nik czyta odpowiedÅº
5. JeÅ›li auto_listen=true â†’ mikrofon znowu siÄ™ wÅ‚Ä…cza
```

## Flow opisu grafiki

```
1. UÅ¼ytkownik mÃ³wi: "opisz tÄ™ stronÄ™"
2. detectIntent â†’ "DESCRIBE"
3. Przechwycenie screenshota WebView (canvas/Tauri cmd) â†’ base64
4. llm.describe(base64png) â†’ OpenRouter (Gemini vision)
5. "WidzÄ™ stronÄ™ z nagÅ‚Ã³wkiem, menu nawigacyjnym, artykuÅ‚em o..."
6. TTS czyta opis
```

---

## KolejnoÅ›Ä‡ implementacji

| # | Zadanie | Pliki | Priorytet |
|---|---------|-------|-----------|
| 1 | `.env` + konfiguracja | `.env`, `vite.config.ts` | ğŸ”´ |
| 2 | `llmClient.ts` + testy | `src/lib/llmClient.ts` | ğŸ”´ |
| 3 | `llm.rs` + komenda Tauri | `src-tauri/src/llm.rs`, `main.rs` | ğŸ”´ |
| 4 | `llmPrompts.ts` | `src/lib/llmPrompts.ts` | ğŸŸ¡ |
| 5 | `useLlm.ts` hook + testy | `src/hooks/useLlm.ts` | ğŸ”´ |
| 6 | Integracja w `Chat.tsx` | `src/components/Chat.tsx` | ğŸ”´ |
| 7 | Detekcja intencji | `llmPrompts.ts` (intent) | ğŸŸ¡ |
| 8 | Screenshot â†’ vision | `screenshot.rs`, `llmClient.ts` | ğŸŸ¢ |
| 9 | Auto-listen loop | `Chat.tsx` + `useSpeech.ts` | ğŸŸ¢ |
| 10 | Streaming odpowiedzi | `llmClient.ts` (SSE) | ğŸŸ¢ |

ğŸ”´ = krytyczne, ğŸŸ¡ = waÅ¼ne, ğŸŸ¢ = nice-to-have

---

## Uwagi

- **Gemini 3 Flash** obsÅ‚uguje multimodal (tekst + obraz) â€” idealne do opisywania stron
- **API key bezpieczeÅ„stwo:** W produkcji klucz TYLKO w Rust backend, nie w frontend
- **Token limit:** TreÅ›Ä‡ strony obcinana do ~6000 znakÃ³w Å¼eby zmieÅ›ciÄ‡ siÄ™ w kontekÅ›cie
- **Koszt:** Gemini Flash jest tani (~$0.10/1M tokenÃ³w) â€” nawet intensywne uÅ¼ycie < $1/dzieÅ„
- **Fallback:** JeÅ›li LLM niedostÄ™pny, Broxeen dziaÅ‚a jak dotychczas (surowy tekst + TTS)
